---
title: "reto2.1"
author: "Adalía Fernanda Aneiros Gutiérrez"
date: "2023-08-30"
output: html_document
---

## **Análisis de interdependencia:**

### Análisis multivariado:

**Lectura de datos**

```{r}
m = read.csv("willyrex.csv")
head(m)
```

```{r}
df = subset(m, select = -c(Planta, X))
head(df)
```

**Matriz de correlaciones:**

```{r}
cor = cor(df)
data.frame(cor)
```

**Heatmap de matriz de correlaciones:**

```{r}

heatmap(cor, main="Heatmap de la matriz de correlaciones", Rowv = NA, Colv = NA)

```

**Matriz de varianzas - covarianzas:**

```{r}
cov = cov(df)
data.frame(cov)
```

Distribución de los datos:

```{r}
boxplot(df)
```

**Comprobación de normalidad en los datos:**

Se utiliza la prueba de Anderson-darling pues se cuenta con una población grande de datos.

$H_0:$ Los datos siguen distribución normal

$H_1:$ Los datos no siguen distribución normal

```{r}
# Prueba de Anderson-Darling
library(nortest)
ad.test(unlist(df))
```

Como el p-value es menor a $\alpha$, se rechaza la hipótesis nula, por lo que se puede suponer que los datos no siguen una distribución normal.

### **Análisis factorial:**

Prueba de KMO para decidir si existe suficiente correlación entre las variables para hacer un análisis factorial:

```{r}
library(psych)
kmo = KMO(df)
kmo

cat("\n Estadístico de prueba resultante:", kmo$MSA)
```

Aunque el valor resultante es mediocre, puede considerarse que existe una correlación considerable entre las variables.

**Implementación del modelo:**

```{r}
library(GPArotation)
fa = fa(cor, nfactors =2, rotate = "varimax", fm ="ml")
fa$loadings

```

ML2 = -0.27(NOX) + 0.798(O3) + 0.335(PM10) - 0.707(RH) + 0.406(SR) + 0.612(TOUT) + 0.518(WSR)

ML1 = 0.960(NOX) + 0.559(PM10) - 0.142(RH) + 0.169(SR) - 0.135(TOUT) - 0.200(WSR) + 0.173(WDR)

**Matriz de datos con Análisis Factorial:**

```{r}

matrizCoef = matrix(c(-0.27, 0.798, 0.335, -0.707, 0.406, 0.612, 0.518, 0, 0.96, 0 , 0.559, -0.142, 0.169, -0.135, -0.2, 0.173), ncol = 2)

matrizOrig =as.matrix(df)
nuevodf = matrizOrig %*% matrizCoef
nuevoDf = data.frame(nuevodf)
nuevoDf
```

**Gráfico de Cattel*:***

```{r}
library(psych)
scree(cor) 
```

## Análisis de dependencia:

### ANOVA

Prueba de hipótesis:

$H_0:$ Las medias de los grupos son todas iguales

$H_1:$ Las medias de los grupos no son todas iguales

**Cálculo de medias por planta:**

```{r}
aggregate(. ~ m$Planta, data = df, FUN = mean)
```

**Implementación del modelo:**

```{r}
Y = m$Planta
X1 = df$NOX
X2 = df$O3
X3 = df$PM10
X4 = df$RH
X5 = df$SR
X6 = df$TOUT
X7 = df$WSR
X8 = df$WDR
anova = aov(Y ~X1+X2+X3+X4+X5+X6+X7+X8)
summary(anova)
```

Como los p-values son menores a 0.05, se rechaza la hipótesis nula, por lo que hay evidencia sucifiente para suponer que las medias de los grupos no son todas iguales, o que al menos hay una que no lo es.

**Boxplots por Componente:**

```{r}
boxplot(formula = NOX~Planta, data = m, title="Boxplot de NOX por Planta")
boxplot(formula = O3~Planta, data = m, title="Boxplot de O3 por Planta")
boxplot(formula = PM10~Planta, data = m, title="Boxplot de PM10 por Planta")
boxplot(formula = RH~Planta, data = m, title="Boxplot de RH por Planta")
boxplot(formula = SR~Planta, data = m, title="Boxplot de SR por Planta")
boxplot(formula = TOUT~Planta, data = m, title="Boxplot de TOUT por Planta")
boxplot(formula = WSR~Planta, data = m, title="Boxplot de WSR por Planta")
boxplot(formula = WDR~Planta, data = m, title="Boxplot de WDR por Planta")
```

**Gráficos:**

```{r}
plot(anova)
```

### 

**Regresión múltiple:**

$H_0:$ No hay relación lineal significativa entre la variable dependiente y las variables predictoras.

$H_1:$ Al menos una de las variables predictoras tiene relación significativa con la variable dependiente.

```{r}

# Regresión múltiple con variables de menor correlación entre ellas
reg = lm(Y~ X1 + X3 + X5 + X8)
summary(reg)
cat ("El modelo de regresión es Y = ", reg$coefficients[1], "+", reg$coefficients[2], "X1 +", reg$coefficients[3], "X3 +", reg$coefficients[4], "X5 +", reg$coefficients[5], "X8")
```

Como el p-value es menor que 0.05, se rechaza la hipótesis nula, por lo que hay evidencia suficiente para suponer que al menos una de las variables predictoras tiene relación significativa con la variable dependiente. Sin embargo, el modelo explica el 9.7% de la varianza total, lo que lo hace un modelo no significativo.

Verificación del modelo:\

-   Media cero:

    $H_0:\mu_r=0$

    $H_1: \mu_r\neq 0$

```{r}
prueba = t.test(reg$residuals, mu = 0, conf.level = 0.95, alternative = "two.sided")
prueba
```

No se rechaza la hipótesis nula por lo que se puede decir que la media de los residuales es cero.

-   Homocedasticidad

    ```{r}
    plot(reg$fitted.values, reg$residuals, col = "blue")
    abline(h=0, col = "red")
    ```

    No se aprecia total homogeneidad en la distribución de los residuales alrededor de la media
    	

    		

    			

    				

    					

    				

    			
